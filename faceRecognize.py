#port 7000 for main compare and detect
import base64
import io
from contextlib import asynccontextmanager
from http.client import HTTPException
import cv2
import numpy as np
from insightface.app import FaceAnalysis
import time
import os
from fastapi import FastAPI,Request
from pydantic import BaseModel
from alibabacloud_facebody20191230.client import Client
from alibabacloud_facebody20191230.models import SearchFaceAdvanceRequest
from alibabacloud_tea_openapi.models import Config
from alibabacloud_tea_util.models import RuntimeOptions
from starlette.responses import JSONResponse
import threading




#ç¡®ä¿åªåˆæ¬¡åŠ è½½æ¨¡å‹ï¼Œé˜²æ­¢åå¤åŠ è½½
@asynccontextmanager
async def lifespan(app: FastAPI):
    face_model = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
    face_model.prepare(ctx_id=-1)

    app.state.face_model = face_model
    app.state.recognition_model = face_model.models['recognition'] #ä½¿ç”¨ ArcFaceONNX
    yield

#æ¨¡å‹åˆå§‹åŒ–
def init_models():
    face_model = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
    face_model.prepare(ctx_id=-1)
    return face_model, face_model.models['recognition']

# FastAPIåˆå§‹åŒ–
app = FastAPI(lifespan=lifespan)
app.state.cached_token = None
app.state.token_expire_time = 0

# æ¥æ”¶è¯·æ±‚ä½“
class ImageInput(BaseModel):
    faceImage: str
    projectCode: str

#å¼‚æ­¥ä¿å­˜
def save_image_async(path, image):
    threading.Thread(target=cv2.imwrite, args=(path, image)).start()

#è½¬æ¢base64ä¸ºå›¾ç‰‡
def base64_to_cv_image(base64_str):
    # å»æ‰å‰ç¼€
    if ',' in base64_str:
        base64_str = base64_str.split(',')[1]
    img_data = base64.b64decode(base64_str)
    np_arr = np.frombuffer(img_data, np.uint8)
    image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
    return image

#tenengradå‡½æ•°
def tenengrad(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # gray = cv2.GaussianBlur(gray, (3, 3), 0.5)  # é™ä½å…‰ç…§å½±å“
    gx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3) #å¯¹xè½´æ±‚å¯¼ï¼Œå·ç§¯æ ¸3*3
    gy = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3) #å¯¹yè½´æ±‚å¯¼ï¼Œå·ç§¯æ ¸3*3
    g = np.sqrt(gx ** 2 + gy ** 2) #è®¡ç®—magnitude
    return np.mean(g) #è®¡ç®—E(X)

#laplacianå‡½æ•°
def laplacian_score(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    return cv2.Laplacian(gray, cv2.CV_64F).var() #æ–¹å·®


# ç»Ÿä¸€å½’ä¸€åŒ–åˆ° 0~100ï¼Œæ ‡å‡†åŒ–3ä¸ªæ–¹æ³•ï¼Œä½¿å…¶åœ¨åŒä¸€ç»´åº¦è¿›è¡Œæ¯”è¾ƒ
def normalize_score(score, min_val, max_val):
    score = np.clip(score, min_val, max_val) # score<min -> 0, score>max -> max
    return 100 * (score - min_val) / (max_val - min_val)

#é¢ç§¯å‡½æ•°
def area_score(image, min_area=60*60, max_area=200*200 , base_score=100.0):
    h, w = image.shape[:2]
    area = h * w
    if area < min_area:
        return 0.0  # é¢ç§¯å¤ªå°ä¸ç»™åˆ†
    area = np.clip(area, min_area, max_area)
    ratio = (area - min_area) / (max_area - min_area)
    bonus_score = ratio * base_score  # æ˜ å°„åˆ° 0 ~ base_score
    return bonus_score

# æ¸…æ™°åº¦æ ¸å¿ƒåˆ¤æ–­
def is_clear(image, threshold=30.0, min_size=50, method='tenengrad',return_score=False):
    h, w = image.shape[:2]
    if h < min_size or w < min_size:
        print("âŒ è¢«åˆ¤ä¸ºæ¨¡ç³Šï¼šå°ºå¯¸è¿‡å°")
        return (False, 0.0) if return_score else False
    score = None

    if method == 'laplacian':
        score = laplacian_score(image)
    elif method == 'tenengrad':
        score = tenengrad(image)
    elif method == 'combined':
        ten = tenengrad(image)
        lap = laplacian_score(image)
        area = area_score(image,base_score=100.0)

        ten_norm = normalize_score(ten, 20, 40)
        lap_norm = normalize_score(lap, 30, 100)
        area_norm = area  # å·²æ˜¯0~100
        #é˜³å…‰ä¸‹lapå®¹æ˜“è™šé«˜ï¼Œæ¯”å€¼è°ƒä½ï¼Œå½“å‰å…¬å¼é€‚åº”å¤§éƒ¨åˆ†æƒ…å†µï¼Œæš‚æ—¶å¿½ç•¥æˆªå–å›¾åˆå¤§åˆä¸æ¸…æ™°è¿™ä¸€æƒ…å†µ
        score = 0.3* ten_norm + 0.1 * lap_norm +0.6*area_norm# æƒé‡æ¯”å€¼
        print(f" ğŸ” Tenengrad: {ten:.2f}, Laplacian: {lap:.2f}, Area: {area:.2f}",)
        print(f" ğŸ‰ Tenengrad_norm: {ten_norm:.2f}, Laplacian-norm: {lap_norm:.2f}, Area_norm: {area_norm:.2f}", )
    else:
        raise ValueError("method must be one of: 'laplacian', 'tenengrad', or 'combined'")

    print(f" ğŸ“ æ¸…æ™°åº¦è¯„åˆ† ({method}): {score:.2f}")
    return (score > threshold, score) if return_score else (score > threshold)


#logæ—¥å¿—è®°å½•è‡ªåŠ¨ç´¯åŠ 
def get_next_face_index(log_dir):
    os.makedirs(log_dir, exist_ok=True)
    existing = [f for f in os.listdir(log_dir) if f.startswith("face_") and f.endswith(".jpg")]
    indexes = []

    for name in existing:
        try:
            idx = int(name.split("_")[1].split(".")[0])
            indexes.append(idx)
        except:
            continue

    return max(indexes) + 1 if indexes else 1


#æ‰©å……æˆªå–çŸ©å½¢ï¼Œé¿å…å›¾åƒè£å‰ªå°è¢«è¿‡æ»¤ï¼Œå›¾åƒä¸è¦åŒ…å«è¾¹æ¡†
def expand_bbox(x1, y1, x2, y2, image_shape, margin=10):
    h, w = image_shape[:2]
    x1 = max(0, x1 - margin)
    y1 = max(0, y1 - margin)
    x2 = min(w, x2 + margin)
    y2 = min(h, y2 + margin)
    return x1, y1, x2, y2

#æ£€æŸ¥å€¾æ–œè§’åº¦æ˜¯å¦ç¬¦åˆæ ‡å‡†
def is_frontal_face(face, image, yaw_thresh=40, pitch_thresh=40, blur_thresh=30.0,
                    method='combined', visualize=False, save_dir="Human_faces", face_index=None):
    yaw, pitch, roll = face.pose
    x1, y1, x2, y2 = map(int, face.bbox)
    x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, image.shape, margin=10)
    face_crop = image[y1:y2, x1:x2]
    print(f"ğŸ”¹ è§’åº¦ yaw={yaw:.2f}, pitch={pitch:.2f}")
    print(f"ğŸ”¹ face_crop å°ºå¯¸: {face_crop.shape[:2]}")

    timestamp = int(time.time() * 1000)
    face_id = f"{face_index}" if face_index is not None else f"{timestamp}"

    # ä¿å­˜ç”¨äºä¸Šä¼ äººè„¸åº“çš„å›¾åƒ
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, f"face_bbox_{face_id}.jpg")
    cv2.imwrite(save_path, face_crop)
    print(f"ğŸ’¾ å·²ä¿å­˜ç”¨äºäººè„¸åº“ä¸Šä¼ ï¼š{save_path}")

    # æ˜¾ç¤ºå›¾åƒèšç„¦ç‚¹ï¼ˆå¯è§†åŒ–ï¼‰
    if visualize:
        img_with_bbox = image.copy()
        cv2.rectangle(img_with_bbox, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.imshow("Face Crop", face_crop)
        cv2.imshow("Image with Face BBox", img_with_bbox)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

    # è¿›è¡Œæ¸…æ™°åº¦åˆ¤æ–­
    is_clear_result,final_score = is_clear(face_crop, threshold=blur_thresh, method=method,return_score=True)
    passed = abs(yaw) < yaw_thresh and abs(pitch) < pitch_thresh and is_clear_result

    # ä¿å­˜æ—¥å¿—å›¾åƒï¼ˆé€šè¿‡å’Œè¿‡æ»¤ï¼‰
    log_dir = "logs/passed_faces" if passed else "logs/filtered_faces"
    os.makedirs(log_dir, exist_ok=True)
    face_index = get_next_face_index(log_dir)
    if face_index is not None:
        log_filename = f"face_{face_index}_{final_score:.2f}_bbox.jpg"
    else:
        log_filename = f"face_{int(time.time() * 1000)}_{final_score:.2f}_bbox.jpg"
    log_path = os.path.join(log_dir, log_filename)
    cv2.imwrite(log_path, face_crop)
    print(f"ğŸ“ æ—¥å¿—å›¾åƒå·²ä¿å­˜è‡³ï¼š{log_path}")
    return passed

#å¤šå€ç‡äººè„¸æ£€æµ‹ï¼Œç”¨æ¥æ£€æŸ¥æ˜¯å¦ä¸ºäººç±»
def detect_faces_with_scaling(image, face_model, scales=(1.0, 1.5, 2.0)):
    for scale in scales:
        resized = cv2.resize(image, None, fx=scale, fy=scale) if scale != 1.0 else image.copy()
        faces = face_model.get(resized)
        if faces:
            print(f"âœ… æ˜¯äººè„¸ï¼ˆç¼©æ”¾å€ç‡ï¼š{scale}xï¼‰")
            return faces, resized, scale
    print("âŒ æ‰€æœ‰ç¼©æ”¾å€ç‡å‡æ£€æµ‹å¤±è´¥ï¼Œä¸åŒ…å«äººè„¸ç‰¹å¾")
    return [], image, 1.0

# def imread_unicode(path):
#     """å…¼å®¹ä¸­æ–‡è·¯å¾„çš„å›¾åƒè¯»å–å‡½æ•°"""
#     stream = np.fromfile(path, dtype=np.uint8)
#     image = cv2.imdecode(stream, cv2.IMREAD_COLOR)
#     return image
#
# def imwrite_unicode(filename, image):
#     """å…¼å®¹ä¸­æ–‡è·¯å¾„çš„å›¾åƒä¿å­˜å‡½æ•°"""
#     ext = os.path.splitext(filename)[1]  # ä¾‹å¦‚ ".jpg"
#     success, encoded_img = cv2.imencode(ext, image)
#     if success:
#         encoded_img.tofile(filename)
#     else:
#         raise ValueError(f"âŒ å›¾åƒç¼–ç å¤±è´¥ï¼Œæ— æ³•ä¿å­˜: {filename}")

#å¦‚æœé€šè¿‡æ£€éªŒå†è°ƒç”¨ç‰¹å¾å‘é‡æå–å’Œæ¯”å¯¹ï¼Œå¦åˆ™ç›´æ¥pass
#æå–äººè„¸ç‰¹å¾å‘é‡

# def extract_embedding(face_img,recognition_model):
#     face_img = cv2.resize(face_img, (112, 112))  #è§„èŒƒåˆ°insightfaceè¾“å…¥å›¾åƒ
#     face_img = cv2.cvtColor(face_img, cv2.COLOR_BGRA2BGR) #è‰²å½©ç©ºé—´è½¬æ¢
#     face_img = np.transpose(face_img, (2, 0, 1))  # (3,112,112)
#     face_img = np.expand_dims(face_img, axis=0)   # (1,3,112,112)
#     face_img = face_img.astype("float32")
#     face_embedding = recognition_model.forward(face_img)[0] #æå–ç‰¹å¾512ç»´å‘é‡
#     return face_embedding / np.linalg.norm(face_embedding) #è®¡ç®—å‘é‡L2å¹³æ–¹å’Œæ ¹å·ï¼Œå³å‘é‡X/||x||,å¾—åˆ°å‘é‡Xçš„æ–¹å‘
#
# #äººè„¸åº“åŸºäºäººå‘˜ç…§ç‰‡åº“/å½“å‰éœ€è¦çš„æ¯”è¾ƒå¯¹è±¡åº“å»è£å‰ª,å¹¶ä¸”ä¿å­˜ç‰¹å¾å‘é‡
# def batch_extract_embeddings(image_dir, db_root_dir, project_name,
#                              face_detector, extract_embedding, recognition_model,
#                              is_cropped=False):
#     db_dir = os.path.join(db_root_dir, project_name)
#     os.makedirs(db_dir, exist_ok=True)
#
#     for filename in os.listdir(image_dir):
#         if not filename.lower().endswith((".jpg", ".jpeg", ".png")):
#             continue
#
#         img_path = os.path.join(image_dir, filename)
#         image = cv2.imread(img_path)
#         if image is None:
#             print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {filename}")
#             continue
#
#         # å¦‚æœå›¾ç‰‡æœªè£å‰ªï¼Œè£å‰ª
#         if not is_cropped:
#             faces = face_detector.get(image)
#             if not faces:
#                 print(f"âŒ æœªæ£€æµ‹åˆ°äººè„¸: {filename}")
#                 continue
#
#             face = faces[0]
#             x1, y1, x2, y2 = map(int, face.bbox)
#             x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, image.shape, margin=10)
#             face_crop = image[y1:y2, x1:x2]
#         else:
#             face_crop = image  # å›¾åƒå·²æ˜¯è£å‰ªäººè„¸
#
#         try:
#             embedding = extract_embedding(face_crop, recognition_model)
#             entity_id = os.path.splitext(filename)[0]
#
#             np.save(os.path.join(db_dir, f"{entity_id}.npy"), embedding)
#             cv2.imwrite(os.path.join(db_dir, f"{entity_id}.jpg"), face_crop)
#
#             print(f"âœ… æˆåŠŸä¿å­˜: {entity_id}")
#         except Exception as e:
#             print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {filename} -> {e}")

#äººè„¸ä½ç½®æ•°ç»„æ›¿ä»£Face_crop,face_cropæ— æ³•ç»Ÿä¸€å›¾ç‰‡å°ºå¯¸åˆ°è®­ç»ƒæ¨¡æ¿ï¼Œç”¨æ¥åšæ£€éªŒåˆé€‚ï¼Œä¸é€‚åˆåšäººè„¸æ¯”å¯¹æå–å‰çš„å¤„ç†
#[x1,y1],[x2,y2]...[x5,y5]
REFERENCE_FIVE_POINTS = np.array([ #5ä¸ªç‰¹å¾ä½ç½®è¶…å‚æ•°ï¼Œå¯è°ƒæ•´
    [38.2946, 51.6963], # å·¦çœ¼ä¸­å¿ƒ
    [73.5318, 51.5014], # å³çœ¼ä¸­å¿ƒ
    [56.0252, 71.7366], # é¼»å°–
    [41.5493, 92.3655], # å·¦å˜´è§’
    [70.7299, 92.2041] # å³å˜´è§’
], dtype=np.float32)

#å°†äº”ä¸ªç‰¹å¾å¯¹é½å›¾åƒäººè„¸ï¼Œè¾“å‡º112*112çš„å›¾
def align_face_by_landmark(image, landmarks, output_size=(112, 112)):
    landmarks = np.array(landmarks, dtype=np.float32)
    tfm = cv2.estimateAffinePartial2D(landmarks, REFERENCE_FIVE_POINTS, method=cv2.LMEDS)[0]
    #è®¡ç®—ä»¿å°„å˜æ¢çŸ©é˜µï¼ŒLMEDSæé«˜å¯¹å¼‚å¸¸ç‚¹çš„å®¹å¿åº¦,[0]å–Mï¼Œp=A*vertical matrix[x y 1]
    aligned = cv2.warpAffine(image, tfm, output_size, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)
    #åº”ç”¨ä»¿å°„å˜æ¢çŸ©é˜µï¼ŒflagåŒçº¿æ€§æ’å€¼ï¼Œå¹³æ»‘ç¼©æ”¾æ•ˆæœï¼ŒborderModeè¶…å‡ºè¾¹ç•Œåƒç´ å¡«å……æ–¹å¼ï¼Œè·å¾—å¯¹é½åçš„å›¾åƒ,
    return aligned

def extract_embedding(face_img, recognition_model):
    face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)  # BGR â†’ RGB
    face_img = cv2.resize(face_img, (112, 112))    #ç»Ÿä¸€åˆ°å°ºå¯¸112ï¼Œ112ï¼Œå’Œè®­ç»ƒæ¨¡å‹ä¸€è‡´
    face_img = np.transpose(face_img, (2, 0, 1))           # CHW
    face_img = np.expand_dims(face_img, axis=0)           # NCHW
    face_img = face_img.astype("float32")
    embedding = recognition_model.forward(face_img)[0]    # æå–ç‰¹å¾
    return embedding / np.linalg.norm(embedding)          # L2å½’ä¸€åŒ–ï¼Œx/||x||å³x^

def batch_extract_embeddings(image_dir, db_root_dir, projectCode,
                             face_detector, recognition_model,entityID,force_override: bool = False,get_entity: bool=False):
    db_dir = os.path.join(db_root_dir, projectCode)
    os.makedirs(db_dir, exist_ok=True)

    for filename in os.listdir(image_dir):
        if not filename.lower().endswith((".jpg", ".jpeg", ".png")):
            continue

        if not get_entity:
            entity_id = os.path.splitext(filename)[0]
            vector_path = os.path.join(db_dir, f"{entity_id}.npy")
        else:
            entity_id = entityID
            vector_path = os.path.join(db_dir, f"{entity_id}.npy")  # âœ… å‘é‡ä¿å­˜è·¯å¾„

        if os.path.exists(vector_path) and not force_override:
            print(f"â­ï¸ å·²å­˜åœ¨å‘é‡æ–‡ä»¶ï¼Œè·³è¿‡: {entity_id}")
            continue

        img_path = os.path.join(image_dir, filename)
        image = cv2.imread(img_path)
        if image is None:
            print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {filename}")
            continue

        # äººè„¸æ£€æµ‹
        faces = face_detector.get(image)
        if not faces:
            print(f"âŒ æœªæ£€æµ‹åˆ°äººè„¸: {filename}")
            continue

        face = faces[0]
        if not hasattr(face, "kps"):
            print(f"âŒ æ— æ³•è·å–äº”ç‚¹å…³é”®ç‚¹: {filename}")
            continue

        try:
            # äº”ç‚¹å¯¹é½ï¼Œface.kpsæ‰¾å½“å‰äººè„¸äº”ç‚¹åæ ‡
            aligned_face = align_face_by_landmark(image, face.kps)

            # æå–ç‰¹å¾å‘é‡
            embedding = extract_embedding(aligned_face, recognition_model)

            # ä¿å­˜å‘é‡ä¸å¯¹é½å›¾åƒ
            np.save(os.path.join(db_dir, f"{entity_id}.npy"), embedding)
            cv2.imwrite(os.path.join(db_dir, f"{entity_id}.jpg"), aligned_face)

            print(f"âœ… æˆåŠŸä¿å­˜: {entity_id}")

        except Exception as e:
            print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {filename} -> {e}")


def map_confidence(sim: float, second_best: float = 0.0) -> float:
    # ä¸æ˜ å°„çš„åŒºåŸŸ
    if sim <= 0.45 or sim >= 0.9:
        return round(sim, 4)

    diff = sim - second_best
    if diff >= 0.3 and sim>=0.4:
        # âœ… æ— è®º sim æ˜¯ä¸æ˜¯é«˜ï¼Œåªè¦å·®è·å¤Ÿå¤§ï¼Œä¹Ÿè®¤ä¸ºå¾ˆè‡ªä¿¡
        mapped = 0.8 + (sim - 0.45) * (0.1 / 0.45)

    elif diff>=0.25:
        mapped = 0.7 + (sim - 0.45) * (0.2 / 0.45)

    elif sim >= 0.45:

        if diff>=0.20:
            mapped = 0.7 + (sim - 0.45) * (0.2 / 0.45)

        elif diff >= 0.15:
            mapped = 0.6 + (sim - 0.45) * (0.3 / 0.45)

        else:
            mapped = 0.5 + (sim - 0.45) * (0.4 / 0.45)
    else:
        # âŒ åˆ†æ•°ä½ã€å·®è·ä¹Ÿä¸å¤§ï¼šä¿ç•™åŸå€¼
        return round(sim, 4)

    return round(mapped, 4)

#1:Näººè„¸æ¯”å¯¹
def compare_faces(query_emb: np.ndarray, db_dir: str, projectCode, top_k=3):
    db_embeddings = [] #ç‰¹å¾å‘é‡
    db_ids = [] #confidenceåºå·
    db_dir_vector = os.path.join(db_dir, projectCode)
    for vector in os.listdir(db_dir_vector):
        if vector.endswith(".npy"):
            emb = np.load(os.path.join(db_dir_vector, vector))#éå†äººè„¸åº“ç‰¹å¾å‘é‡
            db_embeddings.append(emb)
            db_ids.append(os.path.splitext(vector)[0]) #è·å–åˆ†å‰²åçš„å‘é‡åå­—aï¼Œï¼ˆa,.npyï¼‰

    db_matrix = np.vstack(db_embeddings)  # shape: (N, 512)

    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå·²å½’ä¸€åŒ–ï¼Œå¯ç”¨ç‚¹ç§¯ï¼‰
    similarities = db_matrix @ query_emb  # X^*Y^

    # è·å– Top-K ç›¸ä¼¼å‘é‡ç´¢å¼•
    top_k_indices = similarities.argsort()[::-1][:top_k]

    print(f"ğŸ—ï¸\næŸ¥è¯¢äººè„¸å‘é‡æ¯”å¯¹ç»“æœ:")
    result = []
    for i, idx in enumerate(top_k_indices):
        sim = float(similarities[idx])
        if i == 0 and len(top_k_indices) > 1:
            second_best = float(similarities[top_k_indices[1]])
            mapped_sim = map_confidence(sim, second_best=second_best)
        else:
            mapped_sim = round(sim, 4)
        matched_id = db_ids[idx]
        # score: {sim: .4f}
        print(f"  entityId: {matched_id}  confidence:{mapped_sim} DbName: {projectCode}",)
        result.append({"entityId": matched_id, "DbName": projectCode,"confidence":round(mapped_sim,4)})

    return result


#å°†human_facesä¸­çš„å›¾ç‰‡åšç‰¹å¾å‘é‡æå–ä¿å­˜åœ¨å½“å‰æ–‡ä»¶



#æ„å»ºä½™å¼¦ç›¸ä¼¼åº¦å‡½æ•°



#ç‰¹å¾å‘é‡å¾ªç¯db_name ä¸€ä¸€æ¯”å¯¹ï¼Œè¿”å›ç½®ä¿¡åº¦


#å°†è¾“å…¥å›¾ç‰‡æˆªå–åæå–ç‰¹å¾å‘é‡ï¼Œä¸å¯¹åº”äººè„¸æ•°æ®åº“ç¼–å·çš„å‘é‡åº“ä¸­æå–æ‰€æœ‰å‘é‡è¿›è¡Œæ¯”è¾ƒï¼Œk = top3


#åç»­å°†mainæ›¿æ¢ä¸ºinfer
# if __name__ == "__main__":
#     # åˆå§‹åŒ–æ¨¡å‹ï¼ˆCPUï¼‰
#     app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
#     app.prepare(ctx_id=-1)
#
#     # è¯»å–å›¾åƒ
#     img = cv2.imread("image/test34.jpg")
#     if img is None:
#         raise FileNotFoundError("âŒ å›¾åƒè¯»å–å¤±è´¥")
#
#     # å¤šå€ç‡æ£€æµ‹
#     Human_faces, img_rescaled, used_scale = detect_faces_with_scaling(img, app)
#
#     for i, face in enumerate(Human_faces):
#         print(f"\nğŸ” Face #{i + 1}")
#         if is_frontal_face(face, img_rescaled, blur_thresh=25.0, method='tenengrad', visualize=True,face_index=i):
#             print("âœ… æ˜¯æ¸…æ™°æ­£è„¸ï¼ˆå¯ç”¨äºè¯†åˆ«ï¼‰")
#             #å°†å›¾ç‰‡ä¸äººè„¸åº“æ¯”å¯¹åç»­
#         else:
#             print("âŒ éæ­£è„¸ï¼ˆè§’åº¦æˆ–æ¸…æ™°åº¦ä¸è¾¾æ ‡ï¼‰")

#
#è°ƒç”¨äººè„¸åº“

#å®šä¹‰infer
@app.post("/infer")
async def infer(input_data: ImageInput):
    projectCode = input_data.projectCode.strip()
    #æ£€æµ‹ç”¨face-crop,äººè„¸æ¯”å¯¹æå–ç”¨æ ¸å¿ƒäººè„¸ä½ç½®å¯¹é½ï¼Œä¸ä¸å­˜å…¥çš„cropæ£€éªŒå›¾ç‰‡å†²çª.é‡æ–°åˆ›å»ºä½ç½®å­˜åŸå§‹å›¾ç‰‡
    image = base64_to_cv_image(input_data.faceImage)#BASE64è½¬åŒ–å›¾ç‰‡

    if image is None:
        raise FileNotFoundError("âŒ å›¾åƒè¯»å–å¤±è´¥")

    face_model, recognition_model = init_models() #åˆå§‹åŒ–æ¨¡å‹

    # å¤šå€ç‡æ£€æµ‹
    faces, img_rescaled, used_scale = detect_faces_with_scaling(image, face_model)
    print("faces çš„ç±»å‹æ˜¯ï¼š", type(faces))
    if not faces:
        return {"ret": 300, "msg": "æ£€æµ‹æœªåŒ…å«äººè„¸ç‰¹å¾"}#è¾“å‡º
    for i, face in enumerate(faces):
        print(f"\nğŸ” Face #{i + 1}")
        # å¯åœ¨è¿™é‡Œè°ƒèŠ‚é˜ˆå€¼ï¼Œé™åˆ¶å³ä½¿lapå’Œtenæ»¡åˆ†ï¼Œä½†å›¾è¿‡äºå°ä¹Ÿæ²¡æœ‰å‚è€ƒæ„ä¹‰
        if is_frontal_face(face, img_rescaled, blur_thresh=40.0, method='combined', visualize=False, face_index=i):
            print("âœ… æ˜¯æ¸…æ™°æ­£è„¸ï¼ˆå¯ç”¨äºè¯†åˆ«ï¼‰")

            # é€šè¿‡æ£€éªŒåå°†åŸå§‹å›¾ç‰‡å­˜å…¥æ–‡ä»¶ï¼Œç”¨äºç‰¹å¾å‘é‡
            save_dir = "face_origin"
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, f"face.jpg")
            cv2.imwrite(save_path, image)
            print(f"ğŸ˜€ å·²ä¿å­˜ç”¨äºäº”ç‚¹ç‰¹å¾æå–ï¼š{save_path}")
            #æ›´æ–°ç‰¹å¾å‘é‡åº“ï¼Œvector_db,å¦‚æœæœ‰æ–°çš„æ•°æ®æº



            #æå–è¾“å…¥å›¾ç‰‡ä¸­çš„ç‰¹å¾å‘é‡ï¼Œface_originæ–‡ä»¶å§‹ç»ˆä¿æŒæœ€æ–°å†…å®¹
            try:
                batch_extract_embeddings(
                    image_dir="face_origin",
                    db_root_dir="face_origin/face_vector",
                    projectCode="",
                    entityID="",
                    face_detector=face_model,
                    recognition_model=recognition_model,
                    force_override=True
                )
                query_embedding = np.load("face_origin/face_vector/face.npy")
                result =compare_faces(query_embedding, db_dir="vector_db", projectCode=projectCode, top_k=5)
                return JSONResponse(content={"ret": 200, "msg": "è¯†åˆ«æˆåŠŸ","data":result},status_code=200)

            except Exception as error:
                print(error)
                return JSONResponse(content={"ret": 500, "msg": "äººè„¸æœç´¢å¤±è´¥"})





            #é€šè¿‡æ£€éªŒï¼Œè°ƒç”¨é˜¿é‡Œäººè„¸åº“
            # config = Config(
            #     access_key_id="",
            #     access_key_secret="",
            #     endpoint='facebody.cn-shanghai.aliyuncs.com',
            #     region_id='cn-shanghai'
            # )
            # runtime_option = RuntimeOptions()
            # image_path = os.path.join("Human_faces", f"face_bbox_{i}.jpg")
            # with open(image_path, 'rb') as stream0:
            #     search_face_request = SearchFaceAdvanceRequest()
            #     search_face_request.image_url_object = stream0
            #     search_face_request.db_name = projectCode
            #     search_face_request.limit = 5
            #
            #     try:
            #         client = Client(config)
            #         response = client.search_face_advance(search_face_request, runtime_option)
            #         print(response.body)
            #         return JSONResponse(content={
            #             "ret": 200,
            #             "msg": "è¯†åˆ«æˆåŠŸ",
            #             "data": response.body.to_map()
            #         }, status_code=200)
            #     except Exception as error:
            #         print(error)
            #         return JSONResponse(content={"ret": 500, "msg": "é˜¿é‡Œäº‘äººè„¸æœç´¢å¤±è´¥"})
        else:
            print("âŒ éæ­£è„¸ï¼ˆè§’åº¦æˆ–æ¸…æ™°åº¦ä¸è¾¾æ ‡ï¼‰")
            return JSONResponse(content={"ret": 300, "msg": "äººè„¸æœªé€šè¿‡æ£€éªŒï¼Œè¿‡äºæ¨¡ç³Šæˆ–éæ­£è„¸"})



